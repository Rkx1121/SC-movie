import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import tensorflow as tf
from tensorflow import keras
import xgboost as xgb

# è®¾ç½® Seaborn é£æ ¼ï¼Œæé«˜å¯è§†åŒ–ç¾è§‚åº¦
sns.set_theme(style="whitegrid")

# **åˆ›å»ºä¿å­˜å›¾ç‰‡çš„æ–‡ä»¶å¤¹**
fig_dir = "fig"
os.makedirs(fig_dir, exist_ok=True)

# **è¯»å–æ•°æ®**
file_path = "./data/data.csv"
df = pd.read_csv(file_path)

# æ•°æ®é¢„å¤„ç†
df = df.dropna()
df["ç´¯è®¡ç¥¨æˆ¿"] = df["ç´¯è®¡ç¥¨æˆ¿"].astype(str).str.replace("äº¿", "").astype(float)
df["äººæ¬¡"] = df["äººæ¬¡"].astype(str).str.replace("ä¸‡", "").astype(float) * 10000
df["æ—¶é—´"] = df["æ—¶é—´"].astype(int)

# è®­ç»ƒé›†: 2021-2023, æµ‹è¯•é›†: 2024
train_df = df[df["æ—¶é—´"].between(2021, 2023)]
test_df = df[df["æ—¶é—´"] == 2024]

# **ç‰¹å¾é€‰æ‹©**
features = ["ç¥¨æˆ¿å æ¯”", "æ’ç‰‡å æ¯”", "åœºå‡äººæ¬¡", "æ’åº§å æ¯”", "é»„é‡‘å æ¯”"]
target = "ç´¯è®¡ç¥¨æˆ¿"

# **ä½¿ç”¨ pd.get_dummies è¿›è¡Œ One-Hot ç¼–ç **
train_df = pd.get_dummies(train_df, columns=["ç”µå½±", "ç‰ˆæœ¬"], drop_first=True)
test_df = pd.get_dummies(test_df, columns=["ç”µå½±", "ç‰ˆæœ¬"], drop_first=True)

# **ä¿æŒç‰¹å¾åˆ—ä¸€è‡´**
missing_cols = set(train_df.columns) - set(test_df.columns)
for col in missing_cols:
    test_df[col] = 0

features.extend([col for col in train_df.columns if col.startswith("ç”µå½±_") or col.startswith("ç‰ˆæœ¬_")])
train_df = train_df[features + [target]]
test_df = test_df[features + [target]]

# **æ•°æ®æ ‡å‡†åŒ–**
X_train, y_train = train_df[features], train_df[target]
X_test, y_test = test_df[features], test_df[target]

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# **æ„å»º MLP æ¨¡å‹**
mlp_model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1)
])
mlp_model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# **è®­ç»ƒ MLP å¹¶è®°å½•è¿‡ç¨‹**
history = mlp_model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=100, batch_size=16, verbose=1)

# **ğŸ“Œ ç»˜åˆ¶å¹¶ä¿å­˜ è®­ç»ƒè¿‡ç¨‹æ›²çº¿**
fig, axes = plt.subplots(1, 2, figsize=(12, 5), dpi=300)

# æŸå¤±æ›²çº¿
axes[0].plot(history.history['loss'], label='Train Loss', color='#1f77b4', linewidth=2)
axes[0].plot(history.history['val_loss'], label='Validation Loss', color='#ff7f0e', linewidth=2, linestyle="--")
axes[0].set_xlabel('Epochs', fontsize=12)
axes[0].set_ylabel('Loss (MSE)', fontsize=12)
axes[0].set_title('MLP Training Loss Curve', fontsize=14, fontweight='bold')
axes[0].legend()

# MAE æ›²çº¿
axes[1].plot(history.history['mae'], label='Train MAE', color='#2ca02c', linewidth=2)
axes[1].plot(history.history['val_mae'], label='Validation MAE', color='#d62728', linewidth=2, linestyle="--")
axes[1].set_xlabel('Epochs', fontsize=12)
axes[1].set_ylabel('Mean Absolute Error', fontsize=12)
axes[1].set_title('MLP Training MAE Curve', fontsize=14, fontweight='bold')
axes[1].legend()

# **ä¿å­˜é«˜æ¸…å›¾ç‰‡**
training_curve_path = os.path.join(fig_dir, "training_curves.png")
plt.savefig(training_curve_path, dpi=300, bbox_inches='tight')
plt.show()
print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {training_curve_path}")

# **MLP é¢„æµ‹**
y_pred_mlp = mlp_model.predict(X_test_scaled)

# **è®¡ç®— MSE å’Œ RÂ²**
mse_mlp = mean_squared_error(y_test, y_pred_mlp)
r2_mlp = r2_score(y_test, y_pred_mlp)
print(f"MLP - MSE: {mse_mlp:.4f}, RÂ²: {r2_mlp:.4f}")

# **XGBoost è®­ç»ƒ**
xgb_model = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=100)
xgb_model.fit(X_train_scaled, y_train)

# **XGBoost é¢„æµ‹**
y_pred_xgb = xgb_model.predict(X_test_scaled)

# **è®¡ç®— MSE å’Œ RÂ²**
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"XGBoost - MSE: {mse_xgb:.4f}, RÂ²: {r2_xgb:.4f}")

# **ğŸ“Œ ç»˜åˆ¶å¹¶ä¿å­˜ XGBoost ç‰¹å¾é‡è¦æ€§**
fig, ax = plt.subplots(figsize=(10, 6), dpi=300)
xgb.plot_importance(xgb_model, importance_type="gain", ax=ax, title="Feature Importance (XGBoost)")
plt.savefig(os.path.join(fig_dir, "feature_importance.png"), dpi=300, bbox_inches='tight')
plt.show()
print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: {os.path.join(fig_dir, 'feature_importance.png')}")




